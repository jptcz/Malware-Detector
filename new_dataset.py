#!/usr/bin/env python

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB, BernoulliNB
from sklearn.svm import SVC
from sklearn import neighbors
from sklearn.tree import DecisionTreeClassifier
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv1D, MaxPooling1D
import pandas as pd
import keras


mode = 'conv_nn'

print("Reading dataset")
dataset = pd.read_csv("MalwareData.csv", sep='|')
input_data = dataset.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = dataset['legitimate'].values

benign_files = dataset[dataset['legitimate'] == 1].count()
malware_files = dataset[dataset['legitimate'] == 0].count()
print("Benign files: {}\nMalware files: {}".format(benign_files[1], malware_files[1]))

probes = input_data.shape[0]
features = input_data.shape[1]
print("Probes: {}\nFeatures: {}".format(probes, features))

# KNN
if mode == 'knn':
    knn_mode = 'features'
    if knn_mode == 'features':
        print("Creating model")
        n = 7
        weight = 'distance'
        metric = 'manhattan'
        score = {}
        print("kNN parameters:\n\tnumber of neighbours: {}\n\tweight: {}\n\tmetric: {}".format(n, weight, metric))
        classifier = neighbors.KNeighborsClassifier(n, weights=weight, metric=metric)
        for k in range(1, 54):
            best_features_data = SelectKBest(f_classif, k=k).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy*100))
            score[k] = float(accuracy)
        max_acc = max(score.values())
        k = None
        for key in score.keys():
            if score[key] == max_acc:
                k = key
                break
        print("Best accuracy: {} for k={} features.".format(max_acc, k))

    elif knn_mode == 'neighbours':
        weight = 'distance'
        metric = 'manhattan'
        score = {}
        print("Creating model")
        for n in range(1, 10):
            print("kNN parameters:\n\tnumber of neighbours: {}\n\tweight: {}\n\tmetric: {}".format(n, weight, metric))
            classifier = neighbors.KNeighborsClassifier(n, weights=weight, metric=metric)
            best_features_data = SelectKBest(f_classif, k=54).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[n] = float(accuracy)
        max_acc = max(score.values())
        n = None
        for key in score.keys():
            if score[key] == max_acc:
                n = key
                break
        print("Best accuracy: {} for n={} neighbours.".format(max_acc, n))

    elif knn_mode == 'weights':
        weights = ['distance', 'uniform']
        metric = 'manhattan'
        n = 3
        score = {}
        print("Creating model")
        for weight in weights:
            print("kNN parameters:\n\tnumber of neighbours: {}\n\tweight: {}\n\tmetric: {}".format(n, weight, metric))
            classifier = neighbors.KNeighborsClassifier(n, weights=weight, metric=metric)
            best_features_data = SelectKBest(f_classif, k=54).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[weight] = float(accuracy)
        max_acc = max(score.values())
        weight = None
        for key in score.keys():
            if score[key] == max_acc:
                weight = key
                break
        print("Best accuracy: {} for weight: {}.".format(max_acc, weight))

    elif knn_mode == 'metrics':
        weight = 'distance'
        metrics = ['chebyshev', 'euclidean', 'manhattan']
        n = 3
        score = {}
        print("Creating model")
        for metric in metrics:
            print("kNN parameters:\n\tnumber of neighbours: {}\n\tweight: {}\n\tmetric: {}".format(n, weight, metric))
            classifier = neighbors.KNeighborsClassifier(n, weights=weight, metric=metric)
            best_features_data = SelectKBest(f_classif, k=54).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[metric] = float(accuracy)
        max_acc = max(score.values())
        metric = None
        for key in score.keys():
            if score[key] == max_acc:
                metric = key
                break
        print("Best accuracy: {} for metric: {}.".format(max_acc, metric))

# Best accuracy: 0.9934225776541493 for k=13 features.

# NAIVE BAYES
elif mode == 'naive_bayes':
    nb_mode = 'features'
    if nb_mode == 'model':
        score = {}
        x_train, x_test, y_train, y_test = train_test_split(input_data, labels)

        print("Creating model")
        model = MultinomialNB()
        print("Training model")
        model.fit(x_train, y_train)
        print("Evaluating model")
        accuracy = model.score(x_test, y_test)
        print("Accuracy: {}%".format(accuracy*100))
        score['MultinomialNB'] = float(accuracy)

        print("Creating model")
        model = GaussianNB()
        print("Training model")
        model.fit(x_train, y_train)
        print("Evaluating model")
        accuracy = model.score(x_test, y_test)
        print("Accuracy: {}%".format(accuracy*100))
        score['GaussianNB'] = float(accuracy)

        print("Creating model")
        model = ComplementNB()
        print("Training model")
        model.fit(x_train, y_train)
        print("Evaluating model")
        accuracy = model.score(x_test, y_test)
        print("Accuracy: {}%".format(accuracy*100))
        score['ComplementNB'] = float(accuracy)

        print("Creating model")
        model = BernoulliNB()
        print("Training model")
        model.fit(x_train, y_train)
        print("Evaluating model")
        accuracy = model.score(x_test, y_test)
        print("Accuracy: {}%".format(accuracy*100))
        score['BernoulliNB'] = float(accuracy)

        max_acc = max(score.values())
        model = None
        for key in score.keys():
            if score[key] == max_acc:
                model = key
                break
        print("Best accuracy: {} for model: {}.".format(max_acc, model))
    elif nb_mode == 'features':
        score = {}
        for k in range(1, features+1):
            best_features_data = SelectKBest(f_classif, k=k).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Creating model")
            model = MultinomialNB()
            print("Training model")
            model.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = model.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy*100))
            score[k] = float(accuracy)
        max_acc = max(score.values())
        k = None
        for key in score.keys():
            if score[key] == max_acc:
                k = key
                break
        print("Best accuracy: {} for k={} features.".format(max_acc, k))
        # Best accuracy: 0.9546534538711173 for k=50 features.

# deep neural network
elif mode == 'mlp':
    score = {}
    print("Deep neural network")
    for k in range(1, features+1):
        best_features_data = SelectKBest(f_classif, k=k).fit_transform(input_data, labels)
        x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels, test_size=0.3)

        print("Creating model")
        model = keras.Sequential([
            keras.layers.Dense(64, input_shape=(k,), activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dropout(0.5),
            keras.layers.Dense(1, activation='sigmoid')
        ])

        print("Compiling model")
        model.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=["accuracy"])

        print("Training model")
        model.fit(x_train, y_train, epochs=10)

        print("Evaluating model")
        loss, accuracy = model.evaluate(x_test, y_test)
        print("Accuracy: {}\n".format(accuracy*100))

        score[k] = float(accuracy)

    max_acc = max(score.values())
    k = None
    for key in score.keys():
        if score[key] == max_acc:
            k = key
            break
    print("Best accuracy: {} for k={} features.".format(max_acc, k))

# SVM
elif mode == 'svm':
    svm_mode = 'kernels'
    kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']
    gammas = ['scale', 'auto', 0.5]
    if svm_mode == 'features':
        score = {}
        for k in [7, 10, 13]:
            print("Creating model for {} best features".format(k))
            best_features_data = SelectKBest(f_classif, k=k).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            classifier = SVC(gamma='auto')
            print("Fitting model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy*100))
            score[k] = float(accuracy)

        max_acc = max(score.values())
        k = None
        for key in score.keys():
            if score[key] == max_acc:
                k = key
                break
        print("Best accuracy: {}% for k={} features.".format(max_acc*100, k))
        # Best accuracy: 97.34585071859063 % for k=5 features.
    elif svm_mode == 'kernels':
        score = {}
        for kernel in kernels:
            # best_features_data = SelectKBest(f_classif, k=15).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(input_data, labels)
            print("Creating model")
            classifier = SVC(kernel=kernel)
            print("Fitting model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[kernel] = float(accuracy)
        max_acc = max(score.values())
        kernel = None
        for key in score.keys():
            if score[key] == max_acc:
                kernel = key
                break
        print("Best accuracy: {}% for kernel: {}.".format(max_acc*100, kernel))
    elif svm_mode == 'gamma':
        score = {}
        for gamma in gammas:
            # best_features_data = SelectKBest(f_classif, k=15).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(input_data, labels)
            print("Creating model")
            classifier = SVC(gamma=gamma)
            print("Fitting model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[gamma] = float(accuracy)
        max_acc = max(score.values())
        gamma = None
        for key in score.keys():
            if score[key] == max_acc:
                gamma = key
                break
        print("Best accuracy: {}% for gamma: {}.".format(max_acc * 100, gamma))

# Decision tree
elif mode == "decision_tree":
    decision_tree_mode = 'features'
    criterions = ['gini', 'entropy']
    splitters = ['best', 'random']
    if decision_tree_mode == 'features':
        score = {}
        for k in range(1, 54):
            best_features_data = SelectKBest(f_classif, k=k).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(best_features_data, labels)
            print("Creating model")
            classifier = DecisionTreeClassifier()
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy*100))
            score[k] = float(accuracy)
        max_acc = max(score.values())
        k = None
        for key in score.keys():
            if score[key] == max_acc:
                k = key
                break
        print("Best accuracy: {}% for k={} features.".format(max_acc*100, k))
        # Best accuracy: 0.9933936022253129 for k=45 features.
    elif decision_tree_mode == 'criterion':
        score = {}
        for criterion in criterions:
            best_features_data = SelectKBest(f_classif, k=45).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(input_data, labels)
            print("Creating model")
            classifier = DecisionTreeClassifier(criterion=criterion)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[criterion] = float(accuracy)
        max_acc = max(score.values())
        criterion = None
        for key in score.keys():
            if score[key] == max_acc:
                criterion = key
                break
        print("Best accuracy: {}% for criterion:{}.".format(max_acc*100, criterion))
    elif decision_tree_mode == 'splitter':
        score = {}
        for splitter in splitters:
            best_features_data = SelectKBest(f_classif, k=45).fit_transform(input_data, labels)
            x_train, x_test, y_train, y_test = train_test_split(input_data, labels)
            print("Creating model")
            criterion = 'gini'
            classifier = DecisionTreeClassifier(criterion=criterion, splitter=splitter)
            print("Training model")
            classifier.fit(x_train, y_train)
            print("Evaluating model")
            accuracy = classifier.score(x_test, y_test)
            print("Accuracy: {}%".format(accuracy * 100))
            score[splitter] = float(accuracy)
        max_acc = max(score.values())
        splitter = None
        for key in score.keys():
            if score[key] == max_acc:
                splitter = key
                break
        print("Best accuracy: {}% for splitter:{}.".format(max_acc*100, splitter))

# Convolutional neural network
elif mode == 'conv_nn':
    x_train, x_test, y_train, y_test = train_test_split(input_data, labels)
    x_train = x_train.reshape(x_train.shape[0], 54, 1)
    x_test = x_test.reshape(x_test.shape[0], 54, 1)
    num_classes = 2
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    model = Sequential()
    model.add(Conv1D(32, kernel_size=5, activation='relu', strides=1, input_shape=(54, 1)))
    model.add(MaxPooling1D(pool_size=1, strides=2))
    model.add(Conv1D(64, kernel_size=5, activation='relu'))
    model.add(MaxPooling1D(pool_size=1))
    model.add(Flatten())
    model.add(Dense(1000, activation='relu'))
    model.add(Dense(2, activation='softmax'))

    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    model.fit(x_train, y_train, epochs=10)
    loss, accuracy = model.evaluate(x_test, y_test)
    print("Accuracy: {}".format(accuracy))
