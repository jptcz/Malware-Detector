#!/usr/bin/env python

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import numpy as np


dataset = pd.read_csv("MalwareData.csv", sep='|')
input_data = dataset.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = dataset['legitimate'].values

print("{}\nStarting cross-validation\n{}".format("-"*100, "-"*100))
acc_per_fold = []
kf = KFold(n_splits=10, shuffle=True)
fold_no = 1
best_features_data = SelectKBest(f_classif, k=13).fit_transform(input_data, labels)

for train, test in kf.split(best_features_data, labels):

    print("Running fold: {}".format(fold_no))

    print("TRAIN: {} TEST: {}".format(train, test))
    x_train, x_test = best_features_data[train], best_features_data[test]
    y_train, y_test = labels[train], labels[test]

    classifier = DecisionTreeClassifier(criterion='gini', splitter='random')
    classifier.fit(x_train, y_train)
    accuracy = classifier.score(x_test, y_test)
    print("Accuracy for fold {}: {}".format(fold_no, accuracy * 100, "%"))

    acc_per_fold.append(accuracy * 100)
    fold_no = fold_no + 1

print("\n{}\nScore per fold\n{}\n".format("-"*100, "-"*100))
for i in range(0, len(acc_per_fold)):
    print("\tFold {} - Accuracy: {}%".format(i, acc_per_fold[i]))
print("\n{}\nAverage scores for all folds:\n{}\n".format("-"*100, "-"*100))
print("\tAccuracy: {} (+-{})".format(np.mean(acc_per_fold), np.std(acc_per_fold)))
